{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LARS - Least-Angle Regression\n",
    "# LARS is a regression technique that is well suited for\n",
    "# high-dimensional problems (p >> n) where p is the columns of\n",
    "# features and n is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use 200 data points and 500 features with low noise and\n",
    "# a small number of informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "reg_data, reg_target = make_regression(n_samples=200,\n",
    "                                       n_features=500,\n",
    "                                      n_informative=10,\n",
    "                                      noise=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.81412941,  0.21159746, -0.32581413, ..., -0.06106478,\n",
       "        -0.18579926, -0.51674261],\n",
       "       [-0.99460396,  0.92340583,  0.43784461, ...,  1.61522657,\n",
       "        -0.80512506, -1.10938947],\n",
       "       [ 0.65629696, -0.49901232,  0.11073364, ..., -0.59480377,\n",
       "         0.43564802, -0.80762681],\n",
       "       [ 1.02784064, -1.50164649, -0.28185622, ...,  1.17514288,\n",
       "        -0.27632164,  0.01769434],\n",
       "       [ 0.66717096,  2.68264737, -0.09947634, ..., -0.61557118,\n",
       "        -0.05875834, -1.00090044]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10 informative features means we should try for 10 non-zero\n",
    "# coefficients in LARS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.2204460492503131e-16, fit_intercept=True,\n",
       "   fit_path=True, n_nonzero_coefs=10, normalize=True, precompute='auto',\n",
       "   verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lars\n",
    "import numpy as np\n",
    "lars = Lars(n_nonzero_coefs=10)\n",
    "lars.fit(reg_data, reg_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lars.coef_ != 0) # make sure we got exactly 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show why smaller number of features is usually better. Train\n",
    "# 2 LARS models (one with 12 features, one with unlimited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.2204460492503131e-16, fit_intercept=True,\n",
       "   fit_path=True, n_nonzero_coefs=12, normalize=True, precompute='auto',\n",
       "   verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_n = 100\n",
    "lars_12 = Lars(n_nonzero_coefs=12)\n",
    "lars_12.fit(reg_data[:train_n], reg_target[:train_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lars_500 = Lars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=3.324e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=3.193e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=3.150e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=3.089e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=2.973e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=2.947e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 9.714e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=2.866e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=2.726e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=2.689e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=2.675e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 8.297e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=2.668e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.644e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.601e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.504e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.461e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.445e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.436e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.398e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.350e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 9.186e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.334e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.333e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.310e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 9.306e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.279e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.275e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.247e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 7.743e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.236e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.224e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.200e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.189e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.188e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.181e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 7.743e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.168e-02, with an active set of 102 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=2.177e-02, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.2204460492503131e-16, fit_intercept=True,\n",
       "   fit_path=True, n_nonzero_coefs=500, normalize=True, precompute='auto',\n",
       "   verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lars_500.fit(reg_data[:train_n], reg_target[:train_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.420578953581156"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see how each feature fits the unknown data:\n",
    "lars_12_predictions = lars_12.predict(reg_data[train_n:])\n",
    "np.mean(np.power(reg_target[train_n:] - lars_12_predictions, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0565357589130682e+32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lars_500_predictions = lars_500.predict(reg_data[train_n:])\n",
    "np.mean(np.power(reg_target[train_n:] - lars_500_predictions, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform Cross Validation on Lars to tune Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LarsCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16, fit_intercept=True,\n",
       "    max_iter=500, max_n_alphas=1000, n_jobs=1, normalize=True,\n",
       "    precompute='auto', verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcv = LarsCV()\n",
    "lcv.fit(reg_data, reg_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lcv.coef_ != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test out this number to make sure it works better\n",
    "# (This is my personal test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of 25, is 4.54693969485\n",
      "mean of 30, is 4.17785635184\n",
      "mean of 35, is 3.75310461702\n",
      "mean of 40, is 3.52610962126\n",
      "mean of 45, is 3.28265527156\n",
      "mean of 50, is 3.11989033741\n",
      "mean of 55, is 2.96234227537\n",
      "mean of 60, is 2.62289884574\n",
      "mean of 65, is 2.49724889759\n",
      "mean of 70, is 2.34349165116\n",
      "mean of 75, is 2.18933019887\n",
      "mean of 80, is 2.01289072628\n",
      "mean of 85, is 1.88027173393\n",
      "mean of 90, is 1.77641313459\n",
      "mean of 95, is 1.63698424804\n"
     ]
    }
   ],
   "source": [
    "for n in range(25, 100, 5):\n",
    "    model = Lars(n_nonzero_coefs=n)\n",
    "    model.fit(reg_data, reg_target)\n",
    "    preds = model.predict(reg_data)\n",
    "    mean = np.mean(np.power(preds - reg_target, 2))\n",
    "    print \"mean of {}, is {}\".format(n, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying to do the above using a GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_nz_c = np.array(range(25, 100, 5))\n",
    "grid = GridSearchCV(estimator=lars, param_grid=dict(n_nonzero_coefs=n_nz_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Lars(copy_X=True, eps=2.2204460492503131e-16, fit_intercept=True,\n",
       "   fit_path=True, n_nonzero_coefs=10, normalize=True, precompute='auto',\n",
       "   verbose=False),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid={'n_nonzero_coefs': array([25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(reg_data, reg_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99957027083602668"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.n_nonzero_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
